{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12080b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from scipy.stats import mode\n",
    "from numba import njit, typed\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, gaussian_kde"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9065d83e",
   "metadata": {},
   "source": [
    "## Used Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "674379b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_noisy_copies(graph, K, f, verbose=False):\n",
    "    \"\"\"\n",
    "    Function that creates K copies with an percentage error of 'f' of a graph.\n",
    "    \n",
    "    Inputs:\n",
    "        graph: nx.DiGraph()\n",
    "            Original graph from which K copies will be created (this will be the true underlying graph).\n",
    "            \n",
    "        K: int\n",
    "            Number of copies to create from the original graph.\n",
    "    \n",
    "        f: float (from 0 to 1, both inclusive)\n",
    "            The parameter 'f' can be thought of as a measure of the \"error\" or \"noise\" in the\n",
    "            observations. A higher 'f' means more noise, resulting in higher probabilities of\n",
    "            altering the graph (adding or removing edges).\n",
    "            \n",
    "        verbose: bool, optional (default: verbose=False)\n",
    "            Set to True if information about the copies is desired. Change it to False otherwise.\n",
    "    \n",
    "    Returns:\n",
    "        obs: list of nx.DiGraph()\n",
    "            List of the K noisy copies that have been created.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    obs = []\n",
    "    nodes = list(graph.nodes())\n",
    "    edges = list(graph.edges())\n",
    "    num_edges = len(edges)\n",
    "    non_edges = [(u,v) for u in nodes for v in nodes if u!=v and not graph.has_edge(u,v)]\n",
    "    \n",
    "    # Calculate the number of edges to swap\n",
    "    swapping_edges = int(f*num_edges)\n",
    "    \n",
    "    for i in range(K):\n",
    "        graph_copy = graph.copy()\n",
    "            \n",
    "        # Randomly take an edge and non-edge in the graph\n",
    "        random_edges = random.sample(edges, k=swapping_edges)\n",
    "        random_non_edges = random.sample(non_edges+random_edges, k=swapping_edges) # Select non-edges from all positions where there's not an edge, including the places where there used to be an edge.\n",
    "\n",
    "        # Swap them.\n",
    "        graph_copy.remove_edges_from(random_edges)\n",
    "        graph_copy.add_edges_from(random_non_edges)              \n",
    "            \n",
    "        # And append the modified graph to the list\n",
    "        obs.append(graph_copy)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Observation {i+1}: {swapping_edges} edges have been swapped.\")\n",
    "    \n",
    "    return obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f2459c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def energy_H(params):\n",
    "    \"\"\"\n",
    "    Mathematical function that expresses the energy of an alignment with a blueprint given all of its parameters.\n",
    "    \n",
    "    Inputs:\n",
    "        params: array\n",
    "            Array that contains some parameters regarding edge and non-edge matching between nodes in the blueprint\n",
    "            and observations, edges and non-edges in the blueprint, number of observations and hyperparameters for\n",
    "            the assumed prior distributions of the probabilities p and q (in this order).\n",
    "            \n",
    "            Expected array:\n",
    "                params = [O_00, O_01, O_10, O_11, n_0, n_1, n_obs, alpha_p, beta_p, alpha_q, beta_q]\n",
    "            \n",
    "    Returns:\n",
    "        H: float\n",
    "            Value of the energy H of the corresponding alignment.\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    O_00, O_01, O_10, O_11, n_0, n_1, K, alpha_p, beta_p, alpha_q, beta_q = params\n",
    "    \n",
    "    H = -(math.lgamma(O_11+beta_q) + math.lgamma(O_10+alpha_q) + math.lgamma(O_00+beta_p) + math.lgamma(O_01+alpha_p) - math.lgamma(K*n_1 + alpha_q + beta_q) - math.lgamma(K*n_0 + alpha_p + beta_p))\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b17df51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def energy_config_H(observations,alignment,L):\n",
    "    \"\"\"\n",
    "    Computes the posterior probability given a set of observations, an alignment and a blueprint L.\n",
    "\n",
    "    Inputs:\n",
    "        observations: list of nx.DiGraph()\n",
    "            A list of observed graphs (one for each observation). Each graph represents a noisy observation of the \n",
    "            underlying structure of the blueprint graph L.\n",
    "\n",
    "        alignment: np.matrix\n",
    "            A matrix where each row corresponds to a particular observation, and each column represents the alignment \n",
    "            of nodes across observations.\n",
    "            \n",
    "            Example:\n",
    "                [[0,2,1]\n",
    "                 [0,1,2]\n",
    "                 [2,1,0]\n",
    "                 [1,0,2]]\n",
    "            This would be an alignment with 4 observations containing three nodes each. In this case,\n",
    "            the nodes 0, 0, 2 and 1 from observations 0, 1, 2 and 3 (respectively) are mapped to\n",
    "            the same node in the blueprint; and so on for the rest of the nodes (columns).\n",
    "            \n",
    "        L: nx.DiGraph()\n",
    "            The blueprint graph representing the underlying true structure of the noisy observations.\n",
    "\n",
    "    Returns:\n",
    "        H: float\n",
    "            The energy of the alignment based on the given observations and blueprint graph.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    list_of_nodes = list(observations[0].nodes())\n",
    "    n_nodes = len(list_of_nodes)\n",
    "    n_obs = len(observations)\n",
    "        \n",
    "    O_00,O_01,O_10,O_11 = 0,0,0,0\n",
    "\n",
    "    all_positions = [(a,b) for a in range(n_nodes) for b in range(n_nodes) if a!=b]\n",
    "    \n",
    "    for i, j in all_positions:\n",
    "        node_i_in_L, node_j_in_L = list_of_nodes[i],list_of_nodes[j]\n",
    "        for k in range(n_obs):\n",
    "            node_i_in_A, node_j_in_A = alignment[k,i],alignment[k,j]\n",
    "            if L.has_edge(node_i_in_L,node_j_in_L):\n",
    "                if observations[k].has_edge(node_i_in_A,node_j_in_A):\n",
    "                    O_11 += 1\n",
    "                else:\n",
    "                    O_10 += 1\n",
    "            else:\n",
    "                if observations[k].has_edge(node_i_in_A,node_j_in_A):\n",
    "                    O_01 += 1\n",
    "                else:\n",
    "                    O_00 += 1   \n",
    "    \n",
    "    n_1 = L.number_of_edges()\n",
    "    n_0 = n_nodes**2-n_nodes-n_1 # Number of non-edges in L.\n",
    "            \n",
    "    alpha_p,beta_p,alpha_q,beta_q = 0.2,10,0.5,5\n",
    "    params = np.array([O_00,O_01,O_10,O_11,n_0,n_1,n_obs,alpha_p,beta_p,alpha_q,beta_q], dtype=np.float64)\n",
    "    H = energy_H(params=params)\n",
    "\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2a3a5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_alignment_by_degree(observations, seeds=None, group_labels=None):\n",
    "    \"\"\"\n",
    "    Initializes an alignment matrix based on node degree, grouping nodes into different categories: seeds, \n",
    "    group-labeled nodes, and nodes with no additional information (aligned by degree). The function returns \n",
    "    both the alignment matrix and the column positions for each node category. The aligned seed nodes will\n",
    "    be placed in the first columns of the alignment matrix, then each group label, and finally the nodes\n",
    "    with no additional information.\n",
    "    \n",
    "    Inputs:\n",
    "        observations: list of nx.DiGraph()\n",
    "            A list of directed graphs representing the observed networks. These are assumed to be\n",
    "            noisy versions of an underlying true structure.\n",
    "   \n",
    "            \n",
    "        seeds: dict, optional (default: seeds=None)\n",
    "            A dictionary where each key corresponds to the observation, and the associated value is a list of seed \n",
    "            nodes from that observation that are pre-aligned with the blueprint graph (i.e. that its true alignment\n",
    "            is known beforehand). These seeds are used to fix node mappings across observations.\n",
    "\n",
    "            Example:\n",
    "                seeds = {0: [0,1], 1: [6,4], 2: [2,2], 3: [5,3], 4: [3,5]}\n",
    "\n",
    "                In this example, nodes 0, 6, 2, 5 and 3 from observation 0, 1, 2, 3 and 4 (respectively) are mapped\n",
    "                to the same nodes in the blueprint (same for the nodes 1, 4, 2, 3 and 5).\n",
    "                These seeds cannot be swapped during the MCMC process.\n",
    "    \n",
    "        group_labels: list of dict, optional (default: group_labels=None)\n",
    "            A list where each element is a dictionary for a specific group label. The dictionary keys are observation \n",
    "            indices, and the values are lists of node IDs that are grouped together. Nodes with the same group label \n",
    "            across observations can only be mapped to each other.\n",
    "\n",
    "            Example:\n",
    "                group_labels = [\n",
    "                    {0: [2,3], 1: [2,3], 2: [2,3], 3: [2,3], 4: [2,3]}, \n",
    "                    {0: [4,5,6], 1: [4,5,6], 2: [4,5,6], 3: [4,5,6], 4: [4,5,6]}\n",
    "                ]\n",
    "            \n",
    "            In this example, nodes 2 and 3 from all five observations share the same group label, so they can only \n",
    "            be mapped to each other and not to any other nodes. Similarly, nodes 4, 5, and 6 form another group.\n",
    "                \n",
    "    Returns:\n",
    "        alignment: numpy.ndarray\n",
    "            A matrix where each row corresponds to a particular observation, and each column represents the alignment \n",
    "            of nodes across observations. Nodes are grouped first by seeds, then by group-labeled nodes, and finally\n",
    "            by nodes with no additional information (which are aligned based on their degree).\n",
    "            \n",
    "        positions: dict\n",
    "            A dictionary that tracks the column positions for each node category (seeds, group-labeled nodes, and nodes \n",
    "            without additional information). The keys are the category names, and the values are the column ranges \n",
    "            corresponding to those categories in the 'alignment' matrix.\n",
    "\n",
    "            Example:\n",
    "                positions = {\n",
    "                    \"seeds\": range(0, 2), \n",
    "                    \"group_label_0\": range(2, 4), \n",
    "                    \"group_label_1\": range(4, 7), \n",
    "                    \"no info\": range(7, 10)\n",
    "                }\n",
    "\n",
    "            This indicates that the first two columns correspond to seeds, columns 2 and 3 to group label 0,\n",
    "            columns 4 to 6 to group label 1, and columns 7 to 9 to nodes without additional information.\n",
    "                \n",
    "    \"\"\"\n",
    "    \n",
    "    list_of_nodes = list(observations[0].nodes())\n",
    "    \n",
    "    n_rows = len(observations)\n",
    "    n_cols = len(list_of_nodes)\n",
    "    \n",
    "    # Initialize empty lists for matrices and positions\n",
    "    matrices = []\n",
    "    positions = {}\n",
    "    \n",
    "\n",
    "    # SEEDS\n",
    "    if seeds is not None:\n",
    "        num_seeds = len(seeds[0])  # Assuming same number of seeds across observations\n",
    "        seeds_matrix = np.empty((n_rows, num_seeds), dtype=np.int64)\n",
    "        \n",
    "        for observ, nodes_seeds in seeds.items():\n",
    "            seeds_matrix[observ, :num_seeds] = nodes_seeds  # Each row 'i' gets its corresponding seed alignments\n",
    "        \n",
    "        matrices.append(seeds_matrix)\n",
    "        positions[\"seeds\"] = range(0, num_seeds) # Column positions for seeds\n",
    "\n",
    "\n",
    "    # GROUP LABELS\n",
    "    if group_labels is not None:\n",
    "        for i,group_label in enumerate(group_labels):\n",
    "            num_nodes_label = len(group_label[0])\n",
    "            group_matrix = np.empty((n_rows, num_nodes_label), dtype=np.int64)\n",
    "\n",
    "            col_start = sum(len(m[0]) for m in matrices)  # Start column index for this group\n",
    "            for observ, nodes_label in group_label.items():\n",
    "                group_matrix[observ, :num_nodes_label] = nodes_label  # Fill with group nodes\n",
    "\n",
    "            matrices.append(group_matrix)\n",
    "            positions[f\"group_label_{i}\"] = range(col_start, col_start + num_nodes_label)  # Column positions for this group\n",
    "\n",
    "        \n",
    "    # NODES WITHOUT INFORMATION\n",
    "    no_info = {}\n",
    "    for k in range(n_rows):\n",
    "        nodes_no_info = [node for node in observations[k].nodes() if \n",
    "                        (seeds is None or node not in seeds[k]) and \n",
    "                        (group_labels is None or all(node not in group_labels[l][k] for l in range(len(group_labels))))]\n",
    "\n",
    "        sorted_nodes_degree = sorted(nodes_no_info, key=lambda x: observations[k].degree(x), reverse=True)\n",
    "        no_info[k] = sorted_nodes_degree\n",
    "\n",
    "    # Determine the maximum length for the no_info_matrix\n",
    "    num_no_info = len(no_info[0]) if no_info else 0\n",
    "    no_info_matrix = np.empty((n_rows, num_no_info), dtype=np.int64)\n",
    "\n",
    "    for observ, nodes_no_info in no_info.items():\n",
    "        no_info_matrix[observ, :num_no_info] = nodes_no_info\n",
    "\n",
    "    if num_no_info > 0:\n",
    "        matrices.append(no_info_matrix)\n",
    "        col_start = sum(len(m[0]) for m in matrices[:-1])  # Start index for no_info_matrix.\n",
    "        positions[\"no info\"] = range(col_start, col_start + num_no_info)  # Column positions for no info.\n",
    "\n",
    "    # Concatenate all matrices into one alignment matrix.\n",
    "    alignment = np.concatenate(matrices, axis=1)\n",
    "    \n",
    "    return alignment, positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec2d507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_mapping(alignment,get_percentage=False):\n",
    "    \"\"\"\n",
    "    Updates the last row of the alignment matrix with the most common node \n",
    "    mapping for each node across all observations. If there is a tie, \n",
    "    the entry is set to -1.\n",
    "\n",
    "    Inputs:\n",
    "        alignment: np.matrix\n",
    "            An alignment matrix of shape (n_obs, n_nodes).\n",
    "            \n",
    "        get_percentage: bool, optional (default: get_percentage=False)\n",
    "            Sets if the percentage of correctly aligned nodes is calculated or not.\n",
    "\n",
    "    Returns:\n",
    "        most_common_mapping: array\n",
    "            An array that corresponds to the most common mapping of each node or -1 in case of ties.\n",
    "            \n",
    "            Example:\n",
    "                [1,0,2,-1]\n",
    "                \n",
    "            This shows that the most common mapping among observation for the first node of the blueprint\n",
    "            has been the node 1, for the second node, the node 0, then node 2 and finally, for the last\n",
    "            node of the blueprint, there are two nodes that have been equally aligned to it, therefore\n",
    "            it's a tie and -1 is shown.\n",
    "            \n",
    "    \"\"\"\n",
    "\n",
    "    n_obs = alignment.shape[0]\n",
    "    n_nodes = alignment.shape[1]\n",
    "    \n",
    "    correctly_aligned_nodes = 0\n",
    "    most_common_mapping = []\n",
    "    \n",
    "    for n in range(n_nodes):\n",
    "        counter = {i: 0 for i in range(n_nodes)}\n",
    "        for o in range(n_obs):\n",
    "            node = alignment[o,n]\n",
    "            counter[node] += 1\n",
    "            \n",
    "        # Find the node (or nodes) with the maximum count\n",
    "        max_count = max(counter.values())        \n",
    "        most_common_nodes = [node for node, count in counter.items() if count == max_count]\n",
    "        \n",
    "        if len(most_common_nodes) == 1:\n",
    "            # Include the most common mapping for that blueprint node in the new row.\n",
    "            most_common_mapping.append(most_common_nodes[0])\n",
    "            \n",
    "            if get_percentage:\n",
    "                # iF there's a single node representing the majority of aligned nodes among observations, then these are considered as correctly aligned nodes.\n",
    "                correctly_aligned_nodes += max_count\n",
    "            \n",
    "        else:\n",
    "            # Set the label of that node to NaN\n",
    "            most_common_mapping.append(-1)\n",
    "            \n",
    "            # If there's more than one node being the majority, then all of them are considered to be wrongly aligned.\n",
    "    \n",
    "    if get_percentage:\n",
    "        p = correctly_aligned_nodes*100/(n_obs*n_nodes)\n",
    "#         print(f\"Percentage of correctly aligned nodes: {round(p,2)}%\")\n",
    "        return most_common_mapping,p\n",
    "    \n",
    "    return most_common_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "624dfe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def percentage_well_aligned_nodes(alignment):\n",
    "    \"\"\"\n",
    "    Calculates the percentage of correctly aligned nodes across all observations.\n",
    "    A node is considered correctly aligned if it has a single most common mapping across all observations.\n",
    "    \n",
    "    Inputs:\n",
    "        alignment: np.matrix\n",
    "            An alignment matrix of shape (n_obs, n_nodes).\n",
    "            \n",
    "    Returns:\n",
    "        percentage: float\n",
    "            The percentage of correctly aligned nodes across all nodes and observations.\n",
    "    \"\"\"\n",
    "    n_obs = alignment.shape[0]\n",
    "    n_nodes = alignment.shape[1]\n",
    "    \n",
    "    correctly_aligned_nodes = 0\n",
    "\n",
    "    # Loop through each node to calculate the most common mapping and check for correctness\n",
    "    for n in range(n_nodes):\n",
    "        # Create a counter (list) for the occurrences of each node mapping\n",
    "        counter = np.zeros(n_nodes, dtype=np.int32)\n",
    "        \n",
    "        for o in range(n_obs):\n",
    "            node = alignment[o,n]\n",
    "            counter[node] += 1\n",
    "        \n",
    "        # Find the maximum count\n",
    "        max_count = np.max(counter)\n",
    "        # Check how many nodes have the same count as the maximum\n",
    "        most_common_count = np.sum(counter == max_count)\n",
    "        \n",
    "        if most_common_count == 1:\n",
    "            # If there's a single most common node, it's correctly aligned\n",
    "            correctly_aligned_nodes += max_count\n",
    "    \n",
    "    # Calculate the percentage of correctly aligned nodes\n",
    "    percentage = correctly_aligned_nodes * 100 / (n_obs * n_nodes)\n",
    "    \n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "859144cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_L(E,n_obs):\n",
    "    \"\"\"\n",
    "    Creates a directed graph L based on the adjacency matrix E and a threshold on n_obs.\n",
    "    \n",
    "    Inputs:\n",
    "        E: np.matrix\n",
    "            An adjacency matrix where each entry E[i,j] indicates the number of observations that have the edge (i,j).\n",
    "        n_obs, int\n",
    "            The number of observations; used to determine the threshold for edge inclusion.\n",
    "    \n",
    "    Returns:\n",
    "        L: nx.DiGraph\n",
    "            A directed graph where edges (i, j) are included if E[i,j] >= 0.5*n_obs.\n",
    "            \n",
    "    \"\"\"\n",
    "    \n",
    "    L = nx.DiGraph()\n",
    "    n_nodes = E.shape[0]\n",
    "    L.add_nodes_from(np.arange(n_nodes))\n",
    "    \n",
    "    all_possible_edges = [(a,b) for a in range(n_nodes) for b in range(n_nodes) if a!=b]\n",
    "    edges = []\n",
    "\n",
    "    for i,j in all_possible_edges:\n",
    "        if E[i,j] > 0.5*n_obs:\n",
    "            edges.append((i,j))\n",
    "    \n",
    "    L.add_edges_from(edges)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "880722c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def swap_and_update_vars(alignment,adj_matrices_repl,E_old,O_00_old,O_01_old,O_10_old,O_11_old,n_1_old,list_of_nodes,random_obs_idx,x,y):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Total number of observations and nodes.\n",
    "    n_obs,n_nodes = alignment.shape[0],alignment.shape[1]\n",
    "    \n",
    "#     # Select a random observation and two random nodes from it:\n",
    "\n",
    "#     # Obs:\n",
    "#     random_obs_idx = np.random.randint(0,n_obs)\n",
    "\n",
    "#     # Nodes:\n",
    "#     x = np.random.randint(0, n_nodes)\n",
    "#     y = np.random.choice(np.delete(np.arange(n_nodes), x))  # y = random.choice([i for i in range(n_nodes) if i != random_idx_node_1])\n",
    "\n",
    "#     node_1 = new_alignment[random_obs_idx,x]\n",
    "#     node_2 = new_alignment[random_obs_idx,y]\n",
    "        \n",
    "#     print(f\"Obs: {random_obs_idx}. Nodes: {node_1}, {node_2}. Positions: {x}, {y}\")\n",
    "\n",
    "    # Take the adjacent matrix of the randomly chosen DiGraph from that replica\n",
    "    adj_matrix = adj_matrices_repl[random_obs_idx].copy()\n",
    "\n",
    "    E_permut = E_old.copy() # Using .copy() is important, as later on 'E_old' will be used. Without it, any modification of 'E_permut' would also affect 'E_old', and the previous data would be lost!\n",
    "    E_permut -= adj_matrix\n",
    "\n",
    "    # Swap the rows in the randomly chosen positions.\n",
    "    temp_row = adj_matrix[x, :].copy()\n",
    "    adj_matrix[x, :] = adj_matrix[y, :]\n",
    "    adj_matrix[y, :] = temp_row\n",
    "\n",
    "    # Same with the columns.\n",
    "    temp_col = adj_matrix[:, x].copy()\n",
    "    adj_matrix[:, x] = adj_matrix[:, y]\n",
    "    adj_matrix[:, y] = temp_col\n",
    "\n",
    "    E_permut += adj_matrix\n",
    "    E_new = E_permut\n",
    "\n",
    "    \n",
    "    # The new variables are initialized to the old values, and they will be updated according to the proposed swap.\n",
    "    O_00_new,O_01_new,O_10_new,O_11_new,n_1_new = O_00_old,O_01_old,O_10_old,O_11_old,n_1_old\n",
    "\n",
    "    positions_to_check = [(a,b) for a in range(n_nodes) for b in range(n_nodes) if a!=b and (a==x or a==y or b==x or b==y)]\n",
    "\n",
    "    \n",
    "    # Iterate over the positions of the edges to check\n",
    "    for a,b in positions_to_check:\n",
    "        node_a,node_b = list_of_nodes[a],list_of_nodes[b] # alignments[j][n_obs,a],alignments[j][n_obs,b] \n",
    "#                 print(f\"Observations with the edge ({node_a},{node_b}): {E_new[a,b]}\")\n",
    "\n",
    "        # The new L has the edge:\n",
    "        if E_new[a,b] > 0.5*n_obs:\n",
    "\n",
    "            # The previous L also had the edge:\n",
    "            if E_old[node_a,node_b] > 0.5*n_obs:\n",
    "                O_10_new += E_old[a,b] - E_new[a,b] # number of old edges - number of new edges (as new apparisons of an edge (i,k) in the observations now contributes negatively to O_10, which is the total number of matches between an edge (i,k) being in L but not in all observations).\n",
    "                O_11_new += E_new[a,b] - E_old[a,b] # the opposite! number of new edges - number of old edges (this will tell us the number of new edges (i,k) that exist in the observations that also exist in L).\n",
    "                # print(f\"O_00={O_00_new}, O_01={O_01_new}, O_10={O_10_new}, O_11={O_11_new}\\n\")\n",
    "\n",
    "            # The previous L didn't have the edge:\n",
    "            else:\n",
    "                O_00_new -= n_obs - E_old[a,b] # the observations that previously didn't have the edge (node_i,node_j) (same as L)\n",
    "                O_01_new -= E_old[a,b] # the observations that previously had the edge, in contrast to (the previous) L\n",
    "                O_10_new += n_obs - E_new[a,b] # the observations that now don't have the edge (and now L does contain it)\n",
    "                O_11_new += E_new[a,b] # the observations containing the edge (same as L!)\n",
    "                n_1_new += 1 # The number of edges in the blueprint L increases by 1.\n",
    "                # print(f\"O_00={O_00_new}, O_01={O_01_new}, O_10={O_10_new}, O_11={O_11_new}\\n\")\n",
    "\n",
    "        # The new L doesn't have the edge:\n",
    "        else:\n",
    "\n",
    "            # The previous L had the edge:\n",
    "            if E_old[a,b] > 0.5*n_obs:\n",
    "                O_00_new += n_obs - E_new[a,b] # the observations that now don't have the edge (same as L!) previously didn't have the edge (node_i,node_j) (same as L)\n",
    "                O_01_new += E_new[a,b] # the observations containing the edge (and now L doesn't contain it)\n",
    "                O_10_new -= n_obs - E_old[a,b] # the observations that didn't have the edge (and L previously had it)\n",
    "                O_11_new -= E_old[a,b] # the observations that previously had the edge (same as the previous L)\n",
    "                n_1_new -= 1 # The number of edges in the blueprint L decreases by 1.\n",
    "                # print(f\"O_00={O_00_new}, O_01={O_01_new}, O_10={O_10_new}, O_11={O_11_new}\\n\")\n",
    "\n",
    "            # The previous L neither had the edge:\n",
    "            else:\n",
    "                O_00_new += E_old[a,b] - E_new[a,b] # number of old edges - number of new edges (as new apparisons of an edge (i,k) in the observations now contributes negatively to O_00, which is the total number of matches between an edge (i,k) not being in L and also not being in observations).\n",
    "                O_01_new += E_new[a,b] - E_old[a,b] # the opposite! number of new edges - number of old edges (this will tell us the number of new edges (i,k) that exist in the observations that also exist in L).\n",
    "                # print(f\"O_00={O_00_new}, O_01={O_01_new}, O_10={O_10_new}, O_11={O_11_new}\\n\")\n",
    "                \n",
    "\n",
    "    n_0_new = n_nodes**2-n_nodes-n_1_new # Number of non-edges in L, and n_1_new is the number of edges in (the new) L.\n",
    "\n",
    "    return O_00_new,O_01_new,O_10_new,O_11_new,n_0_new,n_1_new,E_new,adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cb3cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def MCMC(max_mcmc_steps,n_replicas,alignments,adj_matrices,E_old,H_old,O_00_old,O_01_old,O_10_old,O_11_old,n_0_old,n_1_old,list_of_nodes,verbose,H_true,stop):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Total number of observations and nodes.\n",
    "    n_obs,n_nodes = alignments[0].shape[0],alignments[0].shape[1]\n",
    "    \n",
    "    H_min = 99999999\n",
    "    align_min = np.empty((n_obs, n_nodes), dtype=np.int64) # DEBUG? 32?\n",
    "    \n",
    "    alignable = 100 # Alignability as a percentage, so at the beginning it is assumed that we will be able to find the best alignment.\n",
    "    same_or_best_align = False # Has the true alignment or a better one been found?\n",
    "    steps = -1\n",
    "    \n",
    "    k = 1\n",
    "    T = 1.05**np.linspace(0,30,n_replicas)\n",
    "    beta = 1/(k*T)\n",
    "    \n",
    "    info = np.empty((n_replicas, max_mcmc_steps))  # Pre-allocate a NumPy array to save the results for each replica after each MCMC step\n",
    "    percentage_align = np.empty(max_mcmc_steps, dtype=np.float64)\n",
    "    O_00_vals = np.empty(max_mcmc_steps, dtype=np.int64)\n",
    "    O_01_vals = np.empty(max_mcmc_steps, dtype=np.int64)\n",
    "    O_10_vals = np.empty(max_mcmc_steps, dtype=np.int64)\n",
    "    O_11_vals = np.empty(max_mcmc_steps, dtype=np.int64)\n",
    "    \n",
    "    # CURRENTLY DISABLED.\n",
    "#     # Here will be stored the alignments that will be saved every 20 MCMC steps.\n",
    "#     sampled_alignments = []\n",
    "    \n",
    "    # MCMC algorithm with parallel tempering starts.\n",
    "    for i in range(max_mcmc_steps):\n",
    "        for l in range(n_nodes//2 * n_obs):\n",
    "            for j in range(n_replicas):\n",
    "            \n",
    "    #             print(f\"\\n\\nIteration {i}: replica {j} ----->\")\n",
    "    \n",
    "                # Select a random observation and pair of nodes.\n",
    "                random_obs_idx = np.random.randint(0,n_obs)\n",
    "                x,y = np.random.choice(n_nodes, size=2, replace=False)\n",
    "                node_1, node_2 = alignments[j][random_obs_idx,x], alignments[j][random_obs_idx,y]\n",
    "                \n",
    "                # Propose a new alignment and compute its energy H_new.\n",
    "                O_00_new, O_01_new, O_10_new, O_11_new, n_0_new, n_1_new, E_new, adj_matrix = \\\n",
    "                swap_and_update_vars(\n",
    "                    alignment=alignments[j], \n",
    "                    adj_matrices_repl=adj_matrices[j], \n",
    "                    E_old=E_old[j], \n",
    "                    O_00_old=O_00_old[j], \n",
    "                    O_01_old=O_01_old[j], \n",
    "                    O_10_old=O_10_old[j], \n",
    "                    O_11_old=O_11_old[j], \n",
    "                    n_1_old=n_1_old[j], \n",
    "                    list_of_nodes=list_of_nodes,\n",
    "                    random_obs_idx=random_obs_idx,\n",
    "                    x=x,\n",
    "                    y=y\n",
    "                )\n",
    "                                \n",
    "                # Calculate the new energy.\n",
    "                alpha_p,beta_p,alpha_q,beta_q = 0.2,10,0.5,5\n",
    "                params = np.array([O_00_new,O_01_new,O_10_new,O_11_new,n_0_new,n_1_new,n_obs,alpha_p,beta_p,alpha_q,beta_q], dtype=np.float64)\n",
    "                H_new = energy_H(params=params)\n",
    "\n",
    "                # Accept it via the Metropolis algorithm.\n",
    "                p_swapping = min(1,math.exp(beta[j]*(H_old[j]-H_new)))\n",
    "                rand = random.random() if p_swapping < 1 else 0\n",
    "\n",
    "                if p_swapping == 1 or p_swapping > rand:\n",
    "    #                 print(\"Improvement: nodes are swapped.\")\n",
    "                    # Accept the swapping:\n",
    "\n",
    "                    # Accept the change: Update the current iteration to be the newest.\n",
    "                    O_00_old[j],O_01_old[j],O_10_old[j],O_11_old[j],n_0_old[j],n_1_old[j] = O_00_new,O_01_new,O_10_new,O_11_new,n_0_new,n_1_new # DEBUG WARNING!\n",
    "                    E_old[j],H_old[j] = E_new,H_new\n",
    "                    # Update the current alignment.\n",
    "                    alignments[j][random_obs_idx,x],alignments[j][random_obs_idx,y] = node_2,node_1\n",
    "                    adj_matrices[j][random_obs_idx] = adj_matrix\n",
    "\n",
    "                # Otherwise, the swap is rejected, and therefore no changes are made.\n",
    "                \n",
    "                # Save the minimum energy throughout the algorithm and the alignment that produced it.\n",
    "                if H_old[j] < H_min:\n",
    "                    H_min = H_old[j]\n",
    "                    align_min = alignments[j]\n",
    "                \n",
    "                # Check the number of steps done when the true alignment or a better one has been found.\n",
    "                if not same_or_best_align:\n",
    "                    if H_old[j]/n_nodes <= H_true+0.0000001: # Some tolerance, just in case Python makes decimal mistakes.\n",
    "                        steps = i+1\n",
    "                        same_or_best_align = True\n",
    "                    \n",
    "\n",
    "        # After each MCMC Step, propose a collective swap between the alignments in contiguous replicas.\n",
    "        start, end = i%2, n_replicas-1 # POSSIBLE DEBUG: n_replicas-2 + i%2\n",
    "        for repl in range(start,end,2):\n",
    "\n",
    "            p_swapping_replicas = min(1,math.exp(-(beta[repl] - beta[repl+1])*(H_old[repl+1] - H_old[repl])))\n",
    "\n",
    "            rand = random.random()\n",
    "            if p_swapping_replicas > rand:\n",
    "                # Change is accepted (all information about both replicas is swapped).\n",
    "                O_00_old[repl],O_00_old[repl+1] = O_00_old[repl+1],O_00_old[repl]\n",
    "                O_01_old[repl],O_01_old[repl+1] = O_01_old[repl+1],O_01_old[repl]\n",
    "                O_10_old[repl],O_10_old[repl+1] = O_10_old[repl+1],O_10_old[repl]\n",
    "                O_11_old[repl],O_11_old[repl+1] = O_11_old[repl+1],O_11_old[repl]\n",
    "                n_0_old[repl],n_0_old[repl+1] = n_0_old[repl+1],n_0_old[repl]\n",
    "                n_1_old[repl],n_1_old[repl+1] = n_1_old[repl+1],n_1_old[repl]\n",
    "\n",
    "                E_old[repl],E_old[repl+1] = E_old[repl+1],E_old[repl]\n",
    "                H_old[repl],H_old[repl+1] = H_old[repl+1],H_old[repl]\n",
    "\n",
    "                alignments[repl],alignments[repl+1] = alignments[repl+1],alignments[repl]\n",
    "                adj_matrices[repl],adj_matrices[repl+1] = adj_matrices[repl+1],adj_matrices[repl]\n",
    "\n",
    "            else:\n",
    "                # Change isn't accepted.\n",
    "#                 print(f\"Change NOT accepted with p={p_swapping_replicas}\")\n",
    "                pass\n",
    "\n",
    "        # Save the energy per node of each MCMC step and replica.\n",
    "        percentage_align[i] = percentage_well_aligned_nodes(alignment=alignments[0])\n",
    "        O_00_vals[i] = O_00_old[0]\n",
    "        O_01_vals[i] = O_01_old[0]\n",
    "        O_10_vals[i] = O_10_old[0]\n",
    "        O_11_vals[i] = O_11_old[0]\n",
    "        for j in range(n_replicas):\n",
    "            info[j,i] = H_old[j]/n_nodes\n",
    "            if alignable == 100:\n",
    "                if info[j,i] < H_true:\n",
    "                    alignable = 0 # When the energy of an alignment is lower than the true energy, then the latter can't be found.\n",
    "\n",
    "        if verbose:\n",
    "            progress = (i+1)/max_mcmc_steps*100\n",
    "            if round(progress,5) % 5 == 0:\n",
    "                print(\"Progress --->\", round(progress,2),\"%\")\n",
    "        \n",
    "        # ALIGNMENT AT EQUILIBRIUM, CURRENTLY DISABLED.\n",
    "        # Store alignments each 20 MCMC steps of the last 500 from the T=1 replica (assumed to be at equilibrium).\n",
    "#         if i > max_mcmc_steps-500:\n",
    "#             if i%20 == 0:\n",
    "#                 sampled_alignments.append(alignments[0])\n",
    "\n",
    "        if same_or_best_align and stop:\n",
    "            return alignments, info[:, :steps], percentage_align[:steps], O_00_vals[:steps], O_01_vals[:steps], O_10_vals[:steps], O_11_vals[:steps], T, alignable, H_min/n_nodes, align_min, steps\n",
    "    \n",
    "    return alignments, info, percentage_align, O_00_vals, O_01_vals, O_10_vals, O_11_vals, T, alignable, H_min/n_nodes, align_min, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4eff1e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mcmc_paraltemp(obs, sigma, stop=True, folder_name=None, iteration=0, perturb=None, max_mcmc_steps=5000, group_labels=None, seeds=None, n_replicas=20, verbose=True):\n",
    "    \"\"\"\n",
    "    Perform an MCMC with parallel tempering to minimise the energy H = -log(p(L,{pi^k},{A^k})),\n",
    "    i.e. to optimise the posterior distribution p(L,{pi^k}|{A^k}) = exp(-H)/p({A^k}) \\prop \\Gamma(O_{11}+\\beta_q)*\\Gamma(O_{10}+\\alpha_q)/\\Gamma(Kn_1+\\alpha_q+\\beta_q) * \\Gamma(O_{00}+\\beta_p)*\\Gamma(O_{01}+\\alpha_p)/\\Gamma(Kn_0+\\alpha_p+\\beta_p).\n",
    "    The parallel tempering will involve 15 replicas exploring the alignment space at the same time at different temperatures.\n",
    "    \n",
    "    Inputs:\n",
    "        obs: list of nx.DiGraph()\n",
    "            List containing all the graph observations.\n",
    "\n",
    "        max_mcmc_steps: int, optional (default: max_mcmc_steps=5000)\n",
    "            Number of maximum iterations (MCMC steps) to perform.\n",
    "        \n",
    "        L: nx.DiGraph(), optional (default: L=None)\n",
    "            If known, true underlying graph (blueprint) of the noisy observations.\n",
    "            This is used for showing the ground true energy in the graph.\n",
    "\n",
    "        group_labels: list of dict, optional (default: group_labels=None)\n",
    "            ......\n",
    "\n",
    "        seeds: dict, optional (default: seeds=None)\n",
    "            ......\n",
    "            \n",
    "        n_replicas: int, optional (default: n_replicas=20)\n",
    "            ......\n",
    "            \n",
    "        verbose: bool, optional (default: verbose=True)\n",
    "            Set to True to show the progress of the MCMC algorithm. Set to False otherwise.\n",
    "            \n",
    "    Returns:\n",
    "        alignments: list of np.matrix\n",
    "            List containing the best alignment for each replica in the MCMC algorithm.\n",
    "\n",
    "        L: list of nx.DiGraph()\n",
    "            List containing the best blueprint for each replica in the MCMC algorithm.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Time the execution of the algorithm.\n",
    "    start = time.time()\n",
    "        \n",
    "    # Some initial parameters.\n",
    "    n_obs = len(obs)\n",
    "    n_nodes = obs[0].number_of_nodes()\n",
    "    list_of_nodes = np.array(obs[0].nodes())\n",
    "    \n",
    "    # Get the ground true energy H_true.\n",
    "    alignment = np.tile(np.arange(n_nodes), (n_obs,1)).astype(np.int64) # True alignment\n",
    "    adj_matrix = np.array([nx.adjacency_matrix(ob, nodelist=alignment[k]).toarray() for k, ob in enumerate(obs)], dtype=np.int64)\n",
    "    E = np.sum(adj_matrix, axis=0)\n",
    "    H_true = energy_config_H(observations=obs,alignment=alignment,L=create_L(E,n_obs))/n_nodes\n",
    "            \n",
    "    # Matrix that represents the alignment between the nodes in the observations. Initially, they will be aligned by degree.\n",
    "    if perturb is None:\n",
    "        alignment, _ = initialize_alignment_by_degree(observations=obs, seeds=seeds, group_labels=group_labels)\n",
    "    else:\n",
    "        alignment = perturb_alignment(np.tile(np.arange(n_nodes),(n_obs,1)), perturb).astype(np.int64)\n",
    "        \n",
    "    # Initial blueprint graph L\n",
    "    O_00,O_01,O_10,O_11 = np.int64(0),np.int64(0),np.int64(0),np.int64(0)\n",
    "    E_initial = np.zeros([n_nodes,n_nodes], dtype=np.int64) # E_ij = number of observations containing i-->j as an edge in L.\n",
    "\n",
    "    # Initialise the adjacency matrices for each observation, and set the matrix of observations as the sum of the adj. matrices.\n",
    "    adj_matrix = np.array([nx.adjacency_matrix(ob, nodelist=alignment[k]).toarray() for k, ob in enumerate(obs)], dtype=np.int64)\n",
    "    E_initial = np.sum(adj_matrix, axis=0)\n",
    "\n",
    "\n",
    "    n_0,n_1 = np.int64(0),np.int64(0) # Number of non-edges and edges of the initial blueprint, respectively.\n",
    "    all_positions = [(a,b) for a in range(n_nodes) for b in range(n_nodes) if a!=b]\n",
    "    \n",
    "    for i,j in all_positions: # We loop through all possible positions of pairs of nodes.\n",
    "        \n",
    "        # If more than half the observations contain the edge (i,j):\n",
    "        if E_initial[i,j] > 0.5*n_obs:\n",
    "            # The edge is included in the blueprint L.\n",
    "            n_1 += 1\n",
    "            O_11 += E_initial[i,j] # Number of observations that have (i,j) as an edge.\n",
    "            O_10 += n_obs - E_initial[i,j] # Number of obs. not containing that edge (total observations - obs. containing it)\n",
    "        \n",
    "        # If the number of observations containing the edge (i,j) is not the majority,\n",
    "        else:\n",
    "            n_0 += 1\n",
    "            O_01 += E_initial[i,j]\n",
    "            O_00 += n_obs - E_initial[i,j]\n",
    "\n",
    "        \n",
    "    # Calculate the initial energy.\n",
    "    alpha_p,beta_p,alpha_q,beta_q = 0.2,10,0.5,5\n",
    "    params = np.array([O_00,O_01,O_10,O_11,n_0,n_1,n_obs,alpha_p,beta_p,alpha_q,beta_q], dtype=np.float64)\n",
    "    H_initial = energy_H(params=params)\n",
    "    \n",
    "    # Define some parameters for the MCMC parallel tempering phase.\n",
    "    O_00_old, O_01_old, O_10_old, O_11_old, n_0_old, n_1_old = [], [], [], [], [], []\n",
    "    E_old, H_old = [], []\n",
    "    alignments = []\n",
    "    \n",
    "    # This will be a list of lists of matrices, where adj_matrices[4][2] will represent the adj matrix in the 4th replica of the 2nd observation.\n",
    "    adj_matrices = []\n",
    "    \n",
    "    for j in range(n_replicas):\n",
    "        O_00_old.append(O_00)\n",
    "        O_01_old.append(O_01)\n",
    "        O_10_old.append(O_10)\n",
    "        O_11_old.append(O_11)\n",
    "        n_0_old.append(n_0)\n",
    "        n_1_old.append(n_1)\n",
    "        \n",
    "        E_old.append(copy.copy(E_initial))\n",
    "        H_old.append(H_initial)\n",
    "        \n",
    "        alignm = alignment.copy()\n",
    "        alignments.append(alignm)\n",
    "        \n",
    "        adj_mat = adj_matrix.copy()\n",
    "        adj_matrices.append(adj_mat)\n",
    "     \n",
    "    O_00_old = typed.List(O_00_old)\n",
    "    O_01_old = typed.List(O_01_old)\n",
    "    O_10_old = typed.List(O_10_old)\n",
    "    O_11_old = typed.List(O_11_old)\n",
    "\n",
    "    n_0_old = typed.List(n_0_old)\n",
    "    n_1_old = typed.List(n_1_old)\n",
    "\n",
    "    E_old = typed.List(E_old)\n",
    "    H_old = typed.List(H_old)\n",
    "\n",
    "    alignments = typed.List(alignments)\n",
    "    adj_matrices = typed.List(adj_matrices)\n",
    "    \n",
    "    # sampled_alignments, CURRENTLY DISABLED.\n",
    "    alignments, info, percentage_each_step, O_00_vals, O_01_vals, O_10_vals, O_11_vals, T, alignable, H_min, min_alignment, steps = \\\n",
    "    MCMC(\n",
    "        max_mcmc_steps,\n",
    "        n_replicas,\n",
    "        alignments,\n",
    "        adj_matrices,\n",
    "        E_old,\n",
    "        H_old,\n",
    "        O_00_old,\n",
    "        O_01_old,\n",
    "        O_10_old,\n",
    "        O_11_old,\n",
    "        n_0_old,\n",
    "        n_1_old,\n",
    "        list_of_nodes,\n",
    "        verbose,\n",
    "        H_true,\n",
    "        stop\n",
    "    )\n",
    "        \n",
    "    \n",
    "    # Obtain the percentage of correctly aligned nodes of the last alignment in the T=1 replica.\n",
    "    last_alignment = alignments[0]\n",
    "    _, percent_last = most_common_mapping(alignment=last_alignment,get_percentage=True)\n",
    "    \n",
    "    # Obtain the percentage of correctly aligned nodes of the alignment that produced the lowest energy.\n",
    "    _, percent_min = most_common_mapping(alignment=min_alignment,get_percentage=True)\n",
    "    \n",
    "    # ALIGNMENT SAMPLED FROM EQUILIBRIUM CURRENTLY DISABLED.\n",
    "#     # Obtain the percentage of correctly aligned nodes of the alignments sampled in equilibrium.\n",
    "#     equi_alignment = most_common_alignment(sampled_alignments)\n",
    "#     _, percent_equi = most_common_mapping(alignment=equi_alignment,get_percentage=True)\n",
    "    \n",
    "    end = time.time()\n",
    "    \n",
    "\n",
    "    # Create a CSV file where all information will be saved.\n",
    "    file_name = f\"results_{sigma}_{iteration}.csv\"\n",
    "    if folder_name and os.path.exists(folder_name):\n",
    "        file_path = os.path.join(folder_name, file_name)\n",
    "    else:\n",
    "        file_path = file_name  # Save in the current directory\n",
    "\n",
    "    # Write a CSV file with all the information of the run.\n",
    "    with open(file_path, mode=\"w\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "    \n",
    "        # Initial parameters before the run.\n",
    "        file.write(\"-------------Parameters:-------------\\n\")\n",
    "        parameters = [\n",
    "            [\"Sigma\", sigma],\n",
    "            [\"MCMC Steps\", max_mcmc_steps],\n",
    "            [\"Observations\", n_obs],\n",
    "            [\"Replicas\", n_replicas]\n",
    "        ]\n",
    "        writer.writerows(parameters)\n",
    "        \n",
    "        # Some obtained values after the run.\n",
    "        values = [\n",
    "            [\"Percentage of correctly aligned nodes\",percent_last,percent_min], # , percent_equi DISABLED.\n",
    "            [\"Alignability\",alignable],\n",
    "            [\"Steps required to reach the true alignment or find a better one\",steps],\n",
    "            [\"H_min and H_true\",H_min,H_true],\n",
    "            [\"Time\",end-start]\n",
    "        ]\n",
    "        writer.writerows(values)\n",
    "\n",
    "        # Energies.\n",
    "        file.write(\"-------------Energies:-------------\\n\")\n",
    "        # Create DataFrame for all found energies.\n",
    "        stps = max_mcmc_steps if (steps == -1 or not stop) else steps\n",
    "        df_energies = pd.DataFrame(info,index=[f\"Replica {p+1}\" for p in range(n_replicas)],columns=[f\"Step {q+1}\" for q in range(stps)])\n",
    "        df_energies.insert(0,\"Temperatures\",T)\n",
    "        info_column = [f\"Replica {j+1}\" for j in range(n_replicas)]\n",
    "        df_energies.insert(0,\"Info replicas\",info_column)\n",
    "        # Write it in the CSV file.\n",
    "        df_energies.to_csv(file,index=False,header=False)\n",
    "        \n",
    "        # Alignments.\n",
    "        file.write(\"-------------Alignments:-------------\\n\")   \n",
    "        # Last alignment\n",
    "        file.write(\"Last alignment\\n\")\n",
    "        df_align = pd.DataFrame(last_alignment)\n",
    "        df_align.to_csv(file,index=False,header=False)\n",
    "        # Minimum alignment\n",
    "        file.write(\"Minimum alignment\\n\")\n",
    "        df_align = pd.DataFrame(min_alignment)\n",
    "        df_align.to_csv(file,index=False,header=False)\n",
    "        # ALIGNMENT FROM EQUILIBRIUM DISABLED.\n",
    "#         # Average alignment from equilibrium\n",
    "#         file.write(\"Equilibrium alignment\\n\")\n",
    "#         df_align = pd.DataFrame(equi_alignment)\n",
    "#         df_align.to_csv(file,index=False,header=False)\n",
    "\n",
    "        # Additional info.\n",
    "        file.write(\"-------------Additional info:-------------\\n\")\n",
    "        writer.writerow([\"Percentages of the best replica per step\"]+list(percentage_each_step))\n",
    "        writer.writerow([\"O_00\"]+list(O_00_vals))\n",
    "        writer.writerow([\"O_01\"]+list(O_01_vals))\n",
    "        writer.writerow([\"O_10\"]+list(O_10_vals))\n",
    "        writer.writerow([\"O_11\"]+list(O_11_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580514de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdbd149c",
   "metadata": {},
   "source": [
    "## Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbe0c52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_energies_csv(file_path,folder_name=None):\n",
    "    percents, O_00_vals, O_01_vals, O_10_vals, O_11_vals = None, None, None, None, None\n",
    "    with open(file_path, mode='r', newline='') as file:\n",
    "        reader = csv.reader(file)\n",
    "        \n",
    "        T = []\n",
    "        energies = []\n",
    "\n",
    "        # Loop through the rows in the CSV file\n",
    "        for row in reader:\n",
    "            if row[0] == \"Sigma\":\n",
    "                sigma = float(row[1])\n",
    "                \n",
    "            elif row[0] == \"MCMC Steps\":\n",
    "                mcmc_total_steps = int(row[1])\n",
    "            \n",
    "            elif row[0] == \"Replicas\":\n",
    "                n_replicas = int(row[1])\n",
    "                \n",
    "            elif row[0] == \"H_min and H_true\":\n",
    "                H_min,H_true = float(row[1]),float(row[2])\n",
    "                \n",
    "            elif row[0].startswith(\"Replica\"):\n",
    "                T.append(float(row[1]))\n",
    "                energies.append([float(H) for H in row[2:]])\n",
    "                \n",
    "            elif row[0] == \"Percentages of the best replica per step\":\n",
    "                percents = [float(x) for x in row[1:]]\n",
    "                \n",
    "            elif row[0] == \"O_00\":\n",
    "                O_00_vals = [int(x) for x in row[1:]]\n",
    "\n",
    "            elif row[0] == \"O_01\":\n",
    "                O_01_vals = [int(x) for x in row[1:]]\n",
    "\n",
    "            elif row[0] == \"O_10\":\n",
    "                O_10_vals = [int(x) for x in row[1:]]\n",
    "\n",
    "            elif row[0] == \"O_11\":\n",
    "                O_11_vals = [int(x) for x in row[1:]]\n",
    "\n",
    "\n",
    "        steps = len(energies[0])\n",
    "        \n",
    "        # Plot energies.\n",
    "        plt.figure(figsize=(25,10))\n",
    "        plt.title(f\"Energy per Node over MCMC Steps for Different Temperatures at ={sigma}\",fontsize=25)\n",
    "        plt.xlabel(\"MCMC step\",fontsize=18)\n",
    "        plt.ylabel(\"Energy per node\",fontsize=18)\n",
    "\n",
    "        # Plot the true energy as a reference.\n",
    "        plt.axhline(y=H_true, color='blue', linestyle='--', label=\"Ground true energy per node\")\n",
    "\n",
    "        for j in range(n_replicas):\n",
    "\n",
    "            if T[j] == 1.0:\n",
    "                plt.plot(range(steps), energies[j], color='black', label=f'T={T[j]}')\n",
    "            else:\n",
    "                plt.plot(range(steps), energies[j], color='pink', label=f'T={T[j]}')\n",
    "\n",
    "        plt.legend(loc='best')\n",
    "        \n",
    "        if folder_name:\n",
    "            image_path = os.path.join(folder_name, \"plot\" + file_path.split(\"results\")[1].split(\".csv\")[0] + \".png\")\n",
    "        else:\n",
    "            image_path = \"plot\" + file_path.split(\"results\")[1].split(\".csv\")[0] + \".png\"\n",
    "            \n",
    "        plt.savefig(image_path)  # Save as a PNG file.\n",
    "        \n",
    "        \n",
    "        # Plot the percentages per each MCMC step.\n",
    "        if percents is not None:\n",
    "            plt.figure(figsize=(25,10))\n",
    "            plt.title(rf\"% of Correctly Aligned Nodes of $T=1$ Replica per MCMC step with $\\sigma={sigma}$\",fontsize=25)\n",
    "            plt.xlabel(\"MCMC step\",fontsize=18)\n",
    "            plt.ylabel(\"% of Correctly Aligned Nodes\",fontsize=18)\n",
    "            plt.plot(range(steps), percents)\n",
    "            \n",
    "            if folder_name:\n",
    "                image_path = os.path.join(folder_name, \"%_align\" + file_path.split(\"results\")[1].split(\".csv\")[0] + \".png\")\n",
    "            else:\n",
    "                image_path = \"%_align\" + file_path.split(\"results\")[1].split(\".csv\")[0] + \".png\"\n",
    "                \n",
    "            plt.savefig(image_path)  # Save as a PNG file.\n",
    "        \n",
    "        \n",
    "        # Plot the values of O_XY per each MCMC step.\n",
    "        if O_00_vals is not None:\n",
    "            plt.figure(figsize=(25,10))\n",
    "            plt.title(rf\"Values of the $O_{{XY}}$ Parameters of the $T=1$ Replica per MCMC step with $\\sigma={sigma}$\",fontsize=25)\n",
    "            plt.xlabel(\"MCMC step\",fontsize=18)\n",
    "            plt.ylabel(r\"Values of $O_{XY}$\",fontsize=18)\n",
    "            \n",
    "            plt.plot(range(steps), O_00_vals, label=r'$O_{00}$')\n",
    "            plt.plot(range(steps), O_01_vals, label=r'$O_{01}$')\n",
    "            plt.plot(range(steps), O_10_vals, label=r'$O_{10}$')\n",
    "            plt.plot(range(steps), O_11_vals, label=r'$O_{11}$')\n",
    "            \n",
    "            plt.legend(loc='best',fontsize=18)\n",
    "            \n",
    "            if folder_name:\n",
    "                image_path = os.path.join(folder_name, \"O_XY\" + file_path.split(\"results\")[1].split(\".csv\")[0] + \".png\")\n",
    "            else:\n",
    "                image_path = \"O_XY\" + file_path.split(\"results\")[1].split(\".csv\")[0] + \".png\"\n",
    "                \n",
    "            plt.savefig(image_path)  # Save as a PNG file.\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0750a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mcmc_multiple_params(G, sigma_range, n_obs, folder_name, n_iter, max_mcmc_steps):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(folder_name):\n",
    "        os.mkdir(folder_name)\n",
    "        print(f\"Folder '{folder_name}' created.\")\n",
    "    else:\n",
    "        print(f\"Folder '{folder_name}' already exists.\")\n",
    "    \n",
    "    for sigma in sigma_range:\n",
    "        for j in range(n_iter):\n",
    "\n",
    "            # Generate noisy copies of the graph\n",
    "            observations = create_noisy_copies(graph=G, K=n_obs, sigma=sigma, verbose=True)\n",
    "\n",
    "            # Run MCMC parallel tempering\n",
    "            mcmc_paraltemp(obs=observations,sigma=sigma,folder_name=folder_name,iteration=j,max_mcmc_steps=max_mcmc_steps,verbose=False)\n",
    "            \n",
    "            # Plot the graph of its energies\n",
    "            file_path = os.path.join(folder_name, f\"results_{sigma}_{j}.csv\")\n",
    "            plot_energies_csv(file_path,folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "178d2087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectability_plot(folder_name,density=True,detectability=True,avg_steps=True,var_steps=True,avg_H_gaps=True,var_H_gaps=True):\n",
    "    \"\"\"\n",
    "    Plot the percentage of correctly aligned nodes across different sigma values.\n",
    "    \n",
    "    Inputs:\n",
    "        node_range:\n",
    "            ...\n",
    "            \n",
    "        edge_range:\n",
    "            ...\n",
    "            \n",
    "        sigma_range: array \n",
    "            Range of sigma values to iterate over.\n",
    "            \n",
    "        G: nx.DiGraph()\n",
    "            Original graph.\n",
    "            \n",
    "        n_observations: int \n",
    "            Number of noisy copies of the graph.\n",
    "            \n",
    "        n_iter: int, optional (default: n_iter=20)\n",
    "            Number of runs of the MCMC algorithm for each sigma.\n",
    "            \n",
    "        max_mcmc_steps: int, optional (default: max_mcmc_steps=5000) \n",
    "            Maximum MCMC steps.\n",
    "        \n",
    "    Returns:\n",
    "        ......\n",
    "        \n",
    "    \"\"\"    \n",
    "    sigmas = []\n",
    "    last_prob = {}\n",
    "    min_prob = {}\n",
    "    align = {}\n",
    "    steps = {}\n",
    "    H_gaps = {}\n",
    "    \n",
    "    files = [f for f in os.listdir(folder_name) if f.startswith(\"results\")]\n",
    "    sorted_files = sorted(files, key=lambda x: float(x.split(\"_\")[1]))\n",
    "    \n",
    "    for file_name in sorted_files:\n",
    "        if file_name.startswith('results'):\n",
    "            file_path = os.path.join(folder_name, file_name)\n",
    "            processed_flags = {\n",
    "                \"percentage\": False,\n",
    "                \"alignability\": False,\n",
    "                \"steps\": False,\n",
    "                \"H_min_H_true\": False\n",
    "            }\n",
    "            sigma = float(file_path.split(\"_\")[1])\n",
    "            if sigma not in sigmas:\n",
    "                sigmas.append(sigma)\n",
    "                last_prob[sigma],min_prob[sigma],align[sigma],steps[sigma],H_gaps[sigma] = [],[],[],[],[]\n",
    "\n",
    "            with open(file_path, mode=\"r\") as file:\n",
    "                \n",
    "                # Read each file.\n",
    "                reader = csv.reader(file)\n",
    "                for row in reader:\n",
    "                    if row[0] == \"Percentage of correctly aligned nodes\":\n",
    "                        percent_last,percent_min = float(row[1]),float(row[2])\n",
    "                        min_prob[sigma].append(percent_min)\n",
    "                        processed_flags[\"percentage\"] = True\n",
    "\n",
    "                    elif row[0] == \"Alignability\":\n",
    "                        alignable = float(row[1])\n",
    "                        align[sigma].append(alignable)\n",
    "                        processed_flags[\"alignability\"] = True\n",
    "\n",
    "                    elif row[0] == \"Steps required to reach the true alignment or find a better one\":\n",
    "                        processed_flags[\"steps\"] = True\n",
    "                        if int(row[1]) != -1:\n",
    "                            max_steps = int(row[1])\n",
    "                            steps[sigma].append(max_steps)\n",
    "\n",
    "                    elif row[0] == \"H_min and H_true\":\n",
    "                        H_min,H_true = float(row[1]),float(row[2])\n",
    "                        H_gaps[sigma].append(H_min-H_true)\n",
    "                        processed_flags[\"H_min_H_true\"] = True\n",
    "\n",
    "                    # If all conditions have been processed, break the loop\n",
    "                    if all(processed_flags.values()):\n",
    "                        break\n",
    "                        \n",
    "    # Calculate over sigma                \n",
    "    min_prob_plot = [np.mean(prob) for prob in min_prob.values()]\n",
    "    align_plot = [np.mean(alignable) for alignable in align.values()]\n",
    "    avg_steps_plot = [np.mean(stp) for stp in steps.values()]\n",
    "    var_steps_plot = [np.std(stp) for stp in steps.values()]\n",
    "    avg_H_gaps_plot = [np.mean(H_gps) for H_gps in H_gaps.values()]\n",
    "    var_H_gaps_plot = [np.std(H_gps) for H_gps in H_gaps.values()]\n",
    "        \n",
    "\n",
    "    # Detectability plot\n",
    "    if detectability:\n",
    "        plt.title(\"Percentage of correctly aligned nodes\")\n",
    "        plt.xlabel(\"Noise level $f$\")\n",
    "        plt.ylabel(\"Percentage (%)\")\n",
    "        plt.tick_params(axis='both', which='major')\n",
    "        \n",
    "        # Alignment Accuracy\n",
    "        plt.plot(sigmas,min_prob_plot,label='Mean % ()')\n",
    "        \n",
    "        # Upper and Lower Bounds\n",
    "        mins_min_prob = [min(prob) for prob in min_prob.values()]\n",
    "        maxs_min_prob = [max(prob) for prob in min_prob.values()]\n",
    "        std_min_prob_plot = [np.std(prob) for prob in min_prob.values()]\n",
    "        upper_bound = [min(maxs_min_prob[i], min_prob_plot[i]+std_min_prob_plot[i]) for i in range(len(sigmas))]\n",
    "        lower_bound = [max(mins_min_prob[i], min_prob_plot[i]-std_min_prob_plot[i]) for i in range(len(sigmas))]\n",
    "\n",
    "        plt.plot(sigmas, upper_bound, linestyle='--', label=\" + \")\n",
    "        plt.plot(sigmas, lower_bound, linestyle='--', label=\" - \")\n",
    "        plt.fill_between(sigmas, lower_bound, upper_bound, color='blue', alpha=0.2)\n",
    "\n",
    "        # Alignability\n",
    "        #plt.plot(sigmas,align_plot,color='red',label='Alignability')\n",
    "\n",
    "        # Additional Info\n",
    "        plt.legend(loc='best')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        image_path = os.path.join(folder_name, \"finalplot.png\")\n",
    "        plt.savefig(image_path)\n",
    "        plt.show()\n",
    "    \n",
    "    if avg_steps:\n",
    "        # Plot the average number of steps.\n",
    "        plt.title(\"Average number of steps to find the true alignment (or a better one)\")\n",
    "        plt.xlabel(\"Noise level $f$\")\n",
    "        plt.ylabel(\"MCMC Steps\")\n",
    "        plt.tick_params(axis='both', which='major')\n",
    "        plt.plot(sigmas,avg_steps_plot,'.-')\n",
    "\n",
    "        # Overlay distributions at each sigma\n",
    "        if density:\n",
    "            for sigma, stps in steps.items():\n",
    "                if len(stps) > 1 and np.std(stps) > 0: # Ensure there's enough data for KDE\n",
    "                    kde = gaussian_kde(stps)\n",
    "                    x_vals = np.linspace(min(stps), max(stps), 1000) # Generate values for KDE\n",
    "                    y_vals = kde(x_vals)\n",
    "                    # Normalize the distribution to fit within the plot\n",
    "                    y_vals = y_vals / max(y_vals) * 0.0075 # Scale factor (adjust as needed)\n",
    "                    plt.plot(sigma - y_vals, x_vals, color='blue', alpha=0.3)\n",
    "\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        image_path = os.path.join(folder_name, \"avg_steps_plot.png\")\n",
    "        plt.savefig(image_path)\n",
    "        plt.show()\n",
    "    \n",
    "    if var_steps:\n",
    "        # Plot the variance of the number of steps.\n",
    "        plt.title(\"Standard deviation of the number of steps to find the true alignment (or a better one)\")\n",
    "        plt.xlabel(\"Noise level $f$\")\n",
    "        plt.ylabel(\"MCMC Steps\")\n",
    "        plt.tick_params(axis='both', which='major')\n",
    "        plt.plot(sigmas,var_steps_plot,'.-')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        image_path = os.path.join(folder_name, \"std_steps_plot.png\")\n",
    "        plt.savefig(image_path)\n",
    "        plt.show()\n",
    "    \n",
    "    if avg_H_gaps:\n",
    "        # Plot the average difference between the best energy found and the ground true energy.\n",
    "        plt.title(r\"Average of $H_{min} - H_{true}$\")\n",
    "        plt.xlabel(\"Noise level $f$\",fontsize=24)\n",
    "        plt.ylabel(\"Energy (H)\",fontsize=24)\n",
    "        plt.tick_params(axis='both', which='major')\n",
    "        plt.plot(sigmas,avg_H_gaps_plot,'.-')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        image_path = os.path.join(folder_name, \"avg_H_gaps_plot.png\")\n",
    "        plt.savefig(image_path)\n",
    "        plt.show()\n",
    "    \n",
    "    if var_H_gaps:\n",
    "        # Plot the variance difference between the best energy found and the ground true energy.\n",
    "        plt.title(r\"Standard deviation of $H_{min} - H_{true}$\")\n",
    "        plt.xlabel(\"Noise level $f$\")\n",
    "        plt.ylabel(\"Energy (H)\")\n",
    "        plt.tick_params(axis='both', which='major')\n",
    "        plt.plot(sigmas,var_H_gaps_plot,'.-')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        image_path = os.path.join(folder_name, \"std_H_gaps_plot.png\")\n",
    "        plt.savefig(image_path)  # Save as a PNG file.\n",
    "        plt.show()\n",
    "\n",
    "#     print(last_prob_plot)\n",
    "#     print(min_prob_plot)\n",
    "#     print(align_plot)\n",
    "#     print(avg_steps_plot)\n",
    "#     print(var_steps_plot)\n",
    "#     print(avg_H_gaps_plot)\n",
    "#     print(var_H_gaps_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df58a8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ede8d916",
   "metadata": {},
   "source": [
    "## Auxiliary Functions (for other purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d62c0688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(graph_list,L=None,alignment=None):\n",
    "    \"\"\"\n",
    "    Shows the graphs that are present in 'graph_list' and optionally the blueprint L.\n",
    "    \n",
    "    Inputs:\n",
    "        graph_list: list of nx.DiGraph()\n",
    "            List of graphs to be plotted (typically here will go the noisy graph observations).\n",
    "            If only one graph is specified, it must be in between brackets\n",
    "            \n",
    "            Example:\n",
    "                >>> plot_graphs(G)\n",
    "                        this will return an error, so instead do the following:\n",
    "                >>> plot_graphs([G])\n",
    "        \n",
    "        L: nx.DiGraph(), optional (default: L=None)\n",
    "            Graph indicating the blueprint L. It nothing is imputed, then it isn't plotted.\n",
    "            This is useful after MCMC has run, to compare the observations with the obtained blueprint L.\n",
    "            \n",
    "        alignment: np.matrix, optional (default: alignment=None)\n",
    "            A matrix where each row corresponds to a particular observation, and each column represents the alignment \n",
    "            of nodes across observations.\n",
    "            \n",
    "            Example:\n",
    "                [[0,2,1]\n",
    "                 [0,1,2]\n",
    "                 [2,1,0]\n",
    "                 [1,0,2]]\n",
    "            This would be an alignment with 4 observations containing three nodes each. In this case,\n",
    "            the nodes 0, 0, 2 and 1 from observations 0, 1, 2 and 3 (respectively) are mapped to\n",
    "            the same node in the blueprint; and so on for the rest of the nodes (columns).\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "            This function doesn't return anything, but rather plots the observations and the blueprint if specified.\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    n_graphs = len(graph_list)\n",
    "    n_last_graphs = n_graphs % 3\n",
    "    n_groups = 1+(n_graphs-n_last_graphs)//3\n",
    "    n_nodes = graph_list[0].number_of_nodes()\n",
    "    \n",
    "    if L is not None:\n",
    "        graph_list = graph_list+[L]\n",
    "    \n",
    "    pos_initial = nx.spring_layout(graph_list[0]) # The position of the plot of the first observation (A^0) is stored, so that all observations are plotted in the same way (not changing the distribution of the vertices in the plot).\n",
    "    only_positions = [positions for _,positions in pos_initial.items()]\n",
    "    \n",
    "    # Create an 'n_groups'x3 grid\n",
    "    fig, axs = plt.subplots(n_groups, 3, figsize=(12, n_groups*4))\n",
    "    \n",
    "    # Flatten axs in case there's only one row, so axs[i//3, i%3] works\n",
    "    if n_groups == 1:\n",
    "        axs = axs.reshape(1, -1)\n",
    "        \n",
    "    pos_list = []\n",
    "    if alignment is not None:\n",
    "        n_iter = n_graphs\n",
    "        if L is not None:\n",
    "            n_iter += 1\n",
    "        for i in range(n_iter):\n",
    "            pos = {alignment[i][j]: only_positions[j] for j in range(n_nodes)}\n",
    "            pos_list.append(pos)\n",
    "    else:\n",
    "        # If no alignment is provided, use the same position for all graphs\n",
    "        pos_list = [pos_initial] * len(graph_list)\n",
    "            \n",
    "            \n",
    "    # Plot each graph on a separate subplot\n",
    "    for i, graph in enumerate(graph_list):\n",
    "        if i != n_graphs: # The position of the blueprint in 'graph_list'!\n",
    "            axs[i//3, i%3].set_title(f\"Observation {i}\")\n",
    "        else:\n",
    "            axs[i//3, i%3].set_title(f\"Blueprint L\")\n",
    "        \n",
    "        nx.draw(graph, pos=pos_list[i], ax=axs[i//3, i%3], with_labels=True, font_weight='bold', node_color='skyblue', node_size=500, arrows=True)\n",
    "\n",
    "    # Turn off unused subplots, if any\n",
    "    for j in range(n_graphs, n_groups*3):\n",
    "        axs[j//3, j%3].axis(\"off\")\n",
    "\n",
    "    # Adjust layout to avoid overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94689103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def H_trues_per_sigma(iters):\n",
    "    start = time.time()\n",
    "    H_trues = []\n",
    "    for sigma in np.linspace(0,1,41):\n",
    "        sigma = round(sigma,5)\n",
    "        H_tr = []\n",
    "        for _ in range(iters):\n",
    "            G = nx.gnp_random_graph(100, 0.1, directed=True)\n",
    "            obs = create_noisy_copies(graph=G, K=2, sigma=sigma)\n",
    "\n",
    "            alignment = np.tile(np.arange(100), (2,1)) # True alignment\n",
    "            adj_matrix = np.array([nx.adjacency_matrix(ob, nodelist=alignment[k]).toarray() for k, ob in enumerate(obs)], dtype=np.int32)\n",
    "            E = np.sum(adj_matrix, axis=0)\n",
    "\n",
    "            h = energy_config_H(observations=obs,alignment=alignment,L=create_L(E,2))/100\n",
    "            H_tr.append(h)\n",
    "\n",
    "        H_trues.append((sigma,np.mean(H_tr)))\n",
    "\n",
    "    end = time.time()\n",
    "    print(f\"Time: {end-start} seconds.\\n\")\n",
    "    \n",
    "    return H_trues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72b2d724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_alignment(true_alignment, n):\n",
    "    \"\"\"\n",
    "    Generate a perturbed initial alignment by cycling n nodes in the alignment.\n",
    "    \n",
    "    Parameters:\n",
    "        true_alignment (ndarray): Shape (n_obs, n_nodes), the true alignment.\n",
    "        n (int): Number of nodes to cycle.\n",
    "    \n",
    "    Returns:\n",
    "        ndarray: A perturbed alignment.\n",
    "    \"\"\"\n",
    "    perturbed_alignment = true_alignment.copy()  # Avoid modifying original\n",
    "    \n",
    "    n_obs, n_nodes = true_alignment.shape\n",
    "    \n",
    "    # Select n random unique indices\n",
    "    selected_indices = np.random.choice(n_nodes, size=n, replace=False)\n",
    "    \n",
    "    # Cycle the selected nodes\n",
    "    # Create a new list where the last element goes to the first place\n",
    "    cycled_indices = np.roll(selected_indices, shift=1)\n",
    "    \n",
    "    # Swap the nodes\n",
    "    for i in range(n):\n",
    "        perturbed_alignment[1, selected_indices[i]] = true_alignment[1, cycled_indices[i]]\n",
    "\n",
    "    print(f'Number of swapped nodes: {np.sum(true_alignment != perturbed_alignment)}')\n",
    "\n",
    "    return perturbed_alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23d231ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_energies_equi(file_path,folder_name=None):\n",
    "    with open(file_path, mode='r', newline='') as file:\n",
    "        reader = csv.reader(file)\n",
    "        \n",
    "        # Loop through the rows in the CSV file\n",
    "        for row in reader:\n",
    "            if row[0].startswith(\"Steps required\"):\n",
    "                final_steps = int(row[1])\n",
    "                \n",
    "            elif row[0] == \"H_min and H_true\":\n",
    "                H_true = float(row[2])\n",
    "                \n",
    "            elif row[0] == \"Replica 1\":\n",
    "                T = float(row[1])\n",
    "                H = [float(h) for h in row[2:]]\n",
    "                \n",
    "                lower_H = float(row[2])\n",
    "                min_H = [lower_H]\n",
    "                \n",
    "                for h in row[3:]:\n",
    "                    if float(h) < lower_H:\n",
    "                        lower_H = float(h)\n",
    "                    min_H.append(lower_H)\n",
    "        \n",
    "        \n",
    "        final_steps = len(H) if final_steps == -1 else final_steps\n",
    "        # Plot energies.\n",
    "        plt.figure(figsize=(25,10))\n",
    "        plt.title(f\"Energy per Node over MCMC Steps in Equilibrium from Replica at T=1\",fontsize=25)\n",
    "        plt.xlabel(\"MCMC step\",fontsize=18)\n",
    "        plt.ylabel(\"Energy per node\",fontsize=18)\n",
    "        \n",
    "#         plt.axhline(y=H_true, color='blue', linestyle='--', label=\"Ground true energy per node\")\n",
    "        \n",
    "        # Plot the true energy as a reference.\n",
    "        plt.plot(range(1000,11*final_steps//12), H[1000:11*final_steps//12], color='pink', label='Energies in equilibrium')\n",
    "        plt.plot(range(1000,11*final_steps//12), min_H[1000:11*final_steps//12], color='black', label='Lowest energy at the moment')\n",
    "        plt.legend(fontsize=18,loc='best')\n",
    "        \n",
    "        if folder_name:\n",
    "            image_path = os.path.join(folder_name, \"energies_equi\" + file_path.split(\"results\")[1].split(\".csv\")[0] + \".png\")\n",
    "        else:\n",
    "            image_path = \"energies_equi\" + file_path.split(\"results\")[1].split(\".csv\")[0] + \".png\"\n",
    "            \n",
    "        plt.savefig(image_path)  # Save as a PNG file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4e8205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def energies_hist(n_graphs, sigma, n_nodes=100, edge_dens=0.1, n_obs=2, max_mcmc_steps=500000, create_graph=True, obs=None, verbose=True):\n",
    "    \"\"\"    \n",
    "    \"\"\"\n",
    "    \n",
    "    global_data = []\n",
    "    mus = []\n",
    "    sigmas = []\n",
    "    plt.figure(figsize=(20,10))\n",
    "    \n",
    "    for iter_graph in range(n_graphs):\n",
    "        if create_graph:\n",
    "            G = nx.gnp_random_graph(n_nodes, edge_dens, directed=True)\n",
    "            obs = create_noisy_copies(graph=G, K=n_obs, sigma=sigma)\n",
    "    \n",
    "        # Initial parameters.\n",
    "    #     n_obs = len(obs)\n",
    "    #     n_nodes = obs[0].number_of_nodes()\n",
    "        list_of_nodes = np.array(obs[0].nodes())\n",
    "\n",
    "        # Ground true energy per node.\n",
    "        alignment = np.tile(np.arange(n_nodes), (n_obs,1))\n",
    "        adj_matrices = np.array([nx.adjacency_matrix(ob, nodelist=alignment[k]).toarray() for k, ob in enumerate(obs)], dtype=np.int32)\n",
    "        E = np.sum(adj_matrices, axis=0)\n",
    "        H_true = energy_config_H(obs,alignment,create_L(E,n_obs))/n_nodes\n",
    "\n",
    "        # Initial alignment (set to random in this case!)\n",
    "        alignment = np.array([np.random.permutation(n_nodes) for _ in range(n_obs)])\n",
    "        \n",
    "        # Save the adjacency matrices of all the observations.                    \n",
    "        adj_matrices = np.array([nx.adjacency_matrix(ob, nodelist=alignment[k]).toarray() for k, ob in enumerate(obs)], dtype=np.int32)\n",
    "\n",
    "        # Now we will initialise the parameters according to the initial alignment.\n",
    "        E = np.sum(adj_matrices, axis=0)\n",
    "        O_00,O_01,O_10,O_11,n_0,n_1 = 0,0,0,0,0,0\n",
    "        all_positions = [(a,b) for a in range(n_nodes) for b in range(n_nodes) if a!=b]\n",
    "\n",
    "        for i,j in all_positions:\n",
    "            if E[i,j] > 0.5*n_obs:\n",
    "                n_1 += 1\n",
    "                O_11 += E[i,j]\n",
    "                O_10 += n_obs - E[i,j]\n",
    "\n",
    "            else:\n",
    "                n_0 += 1\n",
    "                O_01 += E[i,j]\n",
    "                O_00 += n_obs - E[i,j]\n",
    "\n",
    "\n",
    "        # Calculate the initial energy.\n",
    "        alpha_p,beta_p,alpha_q,beta_q = 0.2,10,0.5,5\n",
    "        params = np.array([O_00,O_01,O_10,O_11,n_0,n_1,n_obs,alpha_p,beta_p,alpha_q,beta_q], dtype=np.float64)\n",
    "        H_initial = energy_H(params=params) / n_nodes\n",
    "\n",
    "        energies = [H_initial]\n",
    "        \n",
    "        if H_initial < 0:\n",
    "            raise ValueError(\"Energy can't be negative!\")\n",
    "\n",
    "        \n",
    "        # MCMC algorithm with parallel tempering starts.\n",
    "        for i in range(max_mcmc_steps-1):\n",
    "\n",
    "            # Select a random observation and pair of nodes.\n",
    "            random_obs_idx = np.random.randint(0,n_obs)\n",
    "            x,y = np.random.choice(n_nodes, size=2, replace=False)\n",
    "            node_1, node_2 = alignment[random_obs_idx,x], alignment[random_obs_idx,y]\n",
    "\n",
    "            # Perform a new alignment, swapping the selected nodes, and calculate its new parameters.\n",
    "            O_00, O_01, O_10, O_11, n_0, n_1, E, adj_matrix = \\\n",
    "            swap_and_update_vars(\n",
    "                alignment=alignment, \n",
    "                adj_matrices_repl=adj_matrices, \n",
    "                E_old=E, \n",
    "                O_00_old=O_00, \n",
    "                O_01_old=O_01, \n",
    "                O_10_old=O_10, \n",
    "                O_11_old=O_11, \n",
    "                n_1_old=n_1, \n",
    "                list_of_nodes=list_of_nodes,\n",
    "                random_obs_idx=random_obs_idx,\n",
    "                x=x,\n",
    "                y=y\n",
    "            )\n",
    "\n",
    "            alignment[random_obs_idx,x],alignment[random_obs_idx,y] = node_2,node_1\n",
    "            adj_matrices[random_obs_idx] = adj_matrix\n",
    "\n",
    "            # Compute the new energy.\n",
    "            params = np.array([O_00,O_01,O_10,O_11,n_0,n_1,n_obs,alpha_p,beta_p,alpha_q,beta_q], dtype=np.float64)\n",
    "            H = energy_H(params=params) / n_nodes\n",
    "\n",
    "            if H < 0:\n",
    "                raise ValueError(\"Energy can't be negative!\")\n",
    "\n",
    "            # Save the energy per node.\n",
    "            energies.append(H)\n",
    "\n",
    "            if verbose and round((i+1)/max_mcmc_steps * 100,6) % 5 == 0:\n",
    "                print(\"Progress --->\", round((i+1)/max_mcmc_steps * 100,2),\"%\")\n",
    "\n",
    "\n",
    "        m, s = norm.fit(energies)\n",
    "        mus.append(m)\n",
    "        sigmas.append(s)\n",
    "        global_data.append(energies)\n",
    "        \n",
    "        if verbose:\n",
    "            # Format and style\n",
    "            sns.set_style(\"whitegrid\")\n",
    "            sns.set_context(\"talk\")\n",
    "            \n",
    "            # KDE Plot from Data\n",
    "            sns.kdeplot(energies, color='dodgerblue', fill=True, linewidth=2.5, label='Empirical KDE')\n",
    "            \n",
    "            # Adjusted Gaussian\n",
    "            x = np.linspace(min(energies), max(energies), 1000)\n",
    "            fitted_pdf = norm.pdf(x, m, s)\n",
    "            plt.plot(x, fitted_pdf, color='limegreen', linestyle='--', linewidth=2.5, label='Fitted Gaussian')\n",
    "             \n",
    "            # Title and axis\n",
    "            plt.title(\"Energy per Node Density Distribution\", fontsize=32, pad=20)\n",
    "            plt.xlabel(\"Energy per node $(H)$\", fontsize=25, labelpad=15)\n",
    "            plt.ylabel(\"Density\", fontsize=25, labelpad=15)\n",
    "            # plt.yscale('log')\n",
    "            \n",
    "            # Spacing, grid and legend\n",
    "            plt.xticks(fontsize=16)\n",
    "            plt.yticks(fontsize=16)\n",
    "            plt.grid(True, which=\"both\", linestyle=\"--\")\n",
    "            plt.legend(fontsize=22, frameon=True, loc='best')\n",
    "            plt.tight_layout()\n",
    "                        \n",
    "            # sns.histplot(energies, bins=n_bins, stat=\"density\", color='dodgerblue', kde=True, alpha=0.6, edgecolor='black', label='Empirical')\n",
    "            # plt.hist(energies, bins=250, color='blue', alpha=0.7)  \n",
    "        #     sns.kdeplot(energies, fill=True, color='blue') \n",
    "            # plt.legend(loc='best',fontsize=20)\n",
    "\n",
    "            # Second plot  zoomed-in tail\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            sns.set_style(\"whitegrid\")\n",
    "            sns.set_context(\"talk\")\n",
    "        \n",
    "            # KDE with xlim\n",
    "            sns.kdeplot(energies, color='dodgerblue', fill=True, linewidth=2.5, label='Empirical KDE')\n",
    "        \n",
    "            # Recompute x and fitted_pdf restricted to [46, 48]\n",
    "            x_zoom = np.linspace(54, 59, 1000)\n",
    "            fitted_pdf_zoom = norm.pdf(x_zoom, m, s)\n",
    "            plt.plot(x_zoom, fitted_pdf_zoom, color='limegreen', linestyle='--', linewidth=2.5, label='Fitted Gaussian')\n",
    "        \n",
    "            # Axes limits for zoom\n",
    "            plt.xlim(54, 59)\n",
    "            plt.yscale('log')\n",
    "\n",
    "            # Optional: Set a reasonable y-limit if the density is small in that region\n",
    "            # You can also inspect `fitted_pdf_zoom` or use plt.ylim(0, 0.02)\n",
    "            # plt.ylim(0, 0.02)\n",
    "        \n",
    "            plt.title(\"Zoom on Tail: Energy per Node Distribution\", fontsize=24, pad=20)\n",
    "            plt.xlabel(\"Energy per node $(H)$\", fontsize=18, labelpad=15)\n",
    "            plt.ylabel(\"Density\", fontsize=18, labelpad=15)\n",
    "        \n",
    "            plt.xticks(fontsize=16)\n",
    "            plt.yticks(fontsize=16)\n",
    "            plt.grid(True, which=\"both\", linestyle=\"--\")\n",
    "            plt.legend(fontsize=16, frameon=True, loc='best')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "\n",
    "        progress = int(100*(iter_graph+1)/n_graphs)\n",
    "        if progress % 5 == 0:\n",
    "            print(f\"{progress}%\")\n",
    "    \n",
    "    global_mu, global_sigma = norm.fit(global_data)\n",
    "    avg_mu, avg_sigma = np.mean(mus), np.mean(sigmas)\n",
    "    \n",
    "    H_min = min(energies)\n",
    "    H_max = max(energies)\n",
    "    plt.savefig(\"GaussianEnergy.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return global_mu, global_sigma, avg_mu, avg_sigma, H_min, H_max, H_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "92334fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gaussian_true_H(graph,sigma_range,itr,max_mcmc_steps=500000,K=2):\n",
    "    \"\"\"\n",
    "    Plots the evolution of energy distribution parameters and ground truth energy\n",
    "    against different noise levels.\n",
    "\n",
    "    Inputs:\n",
    "        graph: nx.DiGraph()\n",
    "            The base graph for generating noisy copies.\n",
    "            \n",
    "        sigma_range: array\n",
    "            The range of sigma values (proportion of swapped edges) to be tested.\n",
    "            \n",
    "        max_mcmc_steps: int\n",
    "            Maximum number of sampled  for each noise level.\n",
    "            \n",
    "        K: int\n",
    "            Number of observations to generate per sigma level.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    \n",
    "    mus_avg,sigmas_avg = [],[]\n",
    "    H_mins_avg, H_maxs_avg = [], []  # avg\n",
    "#     H_mins, H_maxs = [], [] # global\n",
    "    H_trues_avg = []\n",
    "    H_trues_mins, H_trues_maxs = [], []\n",
    "    \n",
    "    for sgm in sigma_range:\n",
    "        print(f\"\\nSigma: {round(sgm,2)}\")\n",
    "        mus, sigmas, H_trues, H_mins, H_maxs = [], [], [], [], [] # last 2: avg\n",
    "#         H_mn = float('inf') # global\n",
    "#         H_mx = 0 # global\n",
    "        H_true_min = float('inf')\n",
    "        H_true_max = 0\n",
    "        for i in range(itr):\n",
    "            print(f\"Iteration: {i+1}\")    \n",
    "            observations = create_noisy_copies(graph=graph,K=K,sigma=sgm)\n",
    "            _, _, mu, sigma, H_min, H_max, H_true = energies_hist(n_graphs=1, sigma=sgm, max_mcmc_steps=max_mcmc_steps, create_graph=False, obs=observations, verbose=False)\n",
    "\n",
    "            mus.append(mu)\n",
    "            sigmas.append(sigma)\n",
    "            H_mins.append(H_min) # avg\n",
    "#             H_mn = min(H_mn,H_min) # global\n",
    "            H_maxs.append(H_max) # avg\n",
    "#             H_mx = max(H_mx,H_max) # global\n",
    "            H_trues.append(H_true)\n",
    "\n",
    "            H_true_min = min(H_true,H_true_min)\n",
    "            H_true_max = max(H_true,H_true_max)\n",
    "                \n",
    "        \n",
    "        mus_avg.append(np.mean(mus))\n",
    "        sigmas_avg.append(np.mean(sigmas))\n",
    "        H_mins_avg.append(np.mean(H_mins))\n",
    "#         H_mins.append(H_mn) # global\n",
    "        H_maxs_avg.append(np.mean(H_maxs)) # avg\n",
    "#         H_maxs.append(H_mx) # global\n",
    "        H_trues_avg.append(np.mean(H_trues)) # avg\n",
    "        \n",
    "        H_trues_mins.append(H_true_min)\n",
    "        H_trues_maxs.append(H_true_max)\n",
    "        \n",
    "        \n",
    "    # Convert lists to numpy arrays for easy calculations\n",
    "    mus_avg = np.array(mus_avg)\n",
    "    sigmas_avg = np.array(sigmas_avg)\n",
    "    H_mins_avg = np.array(H_mins_avg) # avg\n",
    "#     H_mins = np.array(H_mins) # global\n",
    "    H_maxs_avg = np.array(H_maxs_avg) # avg\n",
    "#     H_maxs = np.array(H_maxs) # global\n",
    "    H_trues_avg = np.array(H_trues_avg)\n",
    "    H_trues_mins = np.array(H_trues_mins)\n",
    "    H_trues_maxs = np.array(H_trues_maxs)\n",
    "    \n",
    "    # Plot all the information.\n",
    "    plt.figure(figsize=(25,10))\n",
    "    plt.title(\"Sampled Alignments vs Ground True Energy\",fontsize=28)\n",
    "    plt.xlabel(\"Proportion of changed edges ()\",fontsize=20)\n",
    "    plt.ylabel(\"Energy per Node\",fontsize=20)\n",
    "\n",
    "    # Plot \n",
    "    plt.plot(sigma_range, mus_avg, label=\"\", color=\"blue\")\n",
    "\n",
    "    # Plot  +- \n",
    "    upper_bound_mu = mus_avg + sigmas_avg\n",
    "    lower_bound_mu = mus_avg - sigmas_avg\n",
    "    \n",
    "    plt.plot(sigma_range, upper_bound_mu, color='blue', linestyle='--', label=\" + \")\n",
    "    plt.plot(sigma_range, lower_bound_mu, color='blue', linestyle='--', label=\" - \")\n",
    "    plt.fill_between(sigma_range, lower_bound_mu, upper_bound_mu, color='blue', alpha=0.2)\n",
    "\n",
    "    # Plot H_min, H_max\n",
    "    plt.plot(sigma_range, H_mins_avg, color='green', linestyle='--', label=r\"$H_{min}$\") # avg\n",
    "#     plt.plot(sigma_range, H_mins, color='green', linestyle='--', label=r\"$H_{min}$\") # global\n",
    "    plt.plot(sigma_range, H_maxs_avg, color='green', linestyle='--', label=r\"$H_{max}$\") # avg\n",
    "#     plt.plot(sigma_range, H_maxs, color='green', linestyle='--', label=r\"$H_{max}$\") # global\n",
    "    plt.fill_between(sigma_range, H_mins_avg, H_maxs_avg, color='green', alpha=0.2) # avg\n",
    "#     plt.fill_between(sigma_range, H_mins, H_maxs, color='green', alpha=0.2) # global\n",
    "\n",
    "    # Plot H_true\n",
    "    plt.plot(sigma_range, H_trues_avg, label=r\"$H_{true}$\", color='red', linestyle='-')\n",
    "\n",
    "    # Plot H_true_max, H_true_min\n",
    "    plt.plot(sigma_range, H_trues_mins, color='red', linestyle='--', label=r\"$H_{true,min}$\")\n",
    "    plt.plot(sigma_range, H_trues_maxs, color='red', linestyle='--', label=r\"$H_{true,max}$\")\n",
    "    plt.fill_between(sigma_range, H_trues_mins, H_trues_maxs, color='red', alpha=0.2)\n",
    "    \n",
    "    plt.legend(loc='best',fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bba4ce",
   "metadata": {},
   "source": [
    "## Deprecated Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c97c9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_nodes(alignment, positions, seeds=None):\n",
    "    \"\"\"\n",
    "    Randomly selects two nodes from the same observation in the alignment matrix for swapping. The nodes are selected \n",
    "    from different groups (group-labeled nodes, or nodes with no information) according to the positions \n",
    "    dictionary. Seed nodes are excluded from swapping.\n",
    "\n",
    "    Inputs:\n",
    "        alignment: np.matrix\n",
    "            A matrix where each row corresponds to a particular observation, and each column represents the alignment \n",
    "            of nodes across observations. Nodes are grouped first by seeds, then by group-labeled nodes, and finally\n",
    "            by nodes with no additional information (which are aligned based on their degree).\n",
    "\n",
    "        positions: dict\n",
    "            A dictionary that tracks the column positions for each node category (seeds, group-labeled nodes, and nodes \n",
    "            without additional information). The keys are the category names, and the values are the column ranges \n",
    "            corresponding to those categories in the 'alignment' matrix.\n",
    "\n",
    "            Example:\n",
    "                positions = {\n",
    "                    \"seeds\": range(0, 2), \n",
    "                    \"group_label_0\": range(2, 4), \n",
    "                    \"group_label_1\": range(4, 7), \n",
    "                    \"no info\": range(7, 10)\n",
    "                }\n",
    "\n",
    "            This indicates that the first two columns correspond to seeds, columns 2 and 3 to group label 0,\n",
    "            columns 4 to 6 to group label 1, and columns 7 to 9 to nodes without additional information.\n",
    "\n",
    "        seeds: dict, optional (default: seeds=None)\n",
    "            A dictionary containing the seed nodes for each observation. Seed nodes are pre-aligned and cannot be \n",
    "            selected for swapping. The keys are observation indices, and the values are lists of seed node indices.\n",
    "\n",
    "            Example:\n",
    "                seeds = {0: [0,1], 1: [0,1], 2: [0,1], 3: [0,1], 4: [0,1]}\n",
    "\n",
    "            In this example, nodes 0 and 1 in all observations are seeds and will not be selected for swapping.\n",
    "\n",
    "    Returns:\n",
    "        node_1, node_2: nx.DiGraph() node\n",
    "            The two nodes that have been selected to be swapped at a certain MCMC step.\n",
    "\n",
    "        random_obs_idx: int\n",
    "            The index of the randomly selected observation from which the nodes are chosen. This identifies which row \n",
    "            of the alignment matrix is used.\n",
    "\n",
    "        random_idx_node_1, random_idx_node_2: int\n",
    "            The column indices in the alignment matrix of the two randomly selected nodes. This identifies which columns\n",
    "            of the alignment matrix are used.\n",
    "\n",
    "    Raises:\n",
    "        ValueError:\n",
    "            If a seed node is selected for swapping. The function already handles seeds and doesn't swap them; however\n",
    "            this exception is still raised as a way to check that everything works just fine.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # Number of seeds.\n",
    "    n_seeds = len(seeds[0]) if seeds else 0\n",
    "    \n",
    "    # Total number of observations and nodes.\n",
    "    n_obs,n_nodes = alignment.shape[0],alignment.shape[1]\n",
    "    \n",
    "    # Randomly select an observation.\n",
    "    random_obs_idx = random.randint(0,n_obs-1)\n",
    "\n",
    "    # Randomly select a node index between n_seeds and n_nodes.\n",
    "    random_idx_node_1 = random.randint(n_seeds, n_nodes-1)\n",
    "    \n",
    "    # Determine which group the node belongs to by checking the positions.\n",
    "    for node_group, position_range in positions.items():\n",
    "        if random_idx_node_1 in position_range:\n",
    "            \n",
    "            # SEED:\n",
    "            if node_group == \"seeds\":\n",
    "                raise ValueError(\"A seed has been selected for a swap. However, this shouldn't happen, as seed indices have been removed.\")\n",
    "            \n",
    "            # GROUP_LABEL:\n",
    "            elif node_group.startswith(\"group_label\"):\n",
    "                # Randomly select another index from the same group.\n",
    "                random_idx_node_2 = random.choice([i for i in position_range if i != random_idx_node_1])\n",
    "            \n",
    "            # NO INFO:\n",
    "            else:\n",
    "                # Randomly select another index from the nodes without information.\n",
    "                random_idx_node_2 = random.choice([i for i in position_range if i != random_idx_node_1])\n",
    "                \n",
    "            # Select the two selected nodes for swapping.\n",
    "            node_1 = alignment[random_obs_idx,random_idx_node_1]\n",
    "            node_2 = alignment[random_obs_idx,random_idx_node_2]\n",
    "#             print(f\"Positions {random_idx_node_1},{random_idx_node_2}\")\n",
    "#             print(f\"Observation {random_obs_idx}: nodes {node_1},{node_2}\")\n",
    "    \n",
    "            return node_1,node_2,random_obs_idx,random_idx_node_1,random_idx_node_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19fbf492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_alignment(alignments):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # Stack matrices into a 3D array (n_matrices, n_rows, n_cols)\n",
    "    alignments_stack = np.array(alignments)\n",
    "    \n",
    "    # Calculate the mode along the first axis (across matrices) with keepdims=True\n",
    "    mode_alignments, _ = mode(alignments_stack, axis=0, keepdims=True)\n",
    "    \n",
    "    # 'mode_alignments' returns a 3D array, squeeze it to 2D\n",
    "    return np.squeeze(mode_alignments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01392cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
